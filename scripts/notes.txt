Notes on my experiences with running the various pre-processing scripts.

I am new to pretty much everything involved for gen ai, machine learning, etc. 
so there may be a better way to do a lot of the things I have put together to prep for training a DIT model. 
I tried use multithreading and multiple processes where I could add it to speed up the work. There is room for 
improvement on a lot of that. I plan to work on these scripts to improve them as I get to the point of needed 
to pre process larger data sets.  

I chose around 15k meshes to start with for the fine tuning of the Dora-VAE and the training of a DIT model 
for an image to mesh pipeline. I am still working on designing a DIT model and training pipeline for this. 
The meshes I chose are from a large collection of 3d printable models that I have purchased commercial rights 
for over the last few years. 

Hopefully my notes / scripts will help others who are looking to train a DIT model or pre process a bunch of meshes. 
They work in my environment, but might require some tinkering for using in other setups. You will want to provide 
the correct paths to the scripts or move them to the appropriate folders before running them. 

** To get more of the processing to the correct GPU, open up Windows graphics settings, add program, browse to the 
python.exe within the python env you use to run the scripts, select the 3090 or whatever gpu you want to use for 
running the scripts. Example path to the exe to set: C:\Users\YourName\Anaconda3\envs\render_env\python.exe


Ran all of these on my PC:
OS:         Windows 11 Pro
Processor:	Intel Ultra 9 285K   3.70 GHz
RAM: 	    128 GB 
GPU:        MSI Ventus 3x OC 3090 24GB

These notes all apply to preprocessing the meshes for a 256 resolution.

Running the decoder at octree_depth of 8  - 2.1 GB of GPU RAM 27 seconds
Running the decoder at octree_depth of 9  - 3.9 GB of GPU RAM (had a spike up to 6.1 GB right at the end) 3 1/2 minutes
Running the decoder at octree_depth of 10 - 18.3 GB of GPU RAM (had a spike up to 23.7 GB right at the end) 30 minutes

I have modified the autoencoder to allow overriding the test_step to export the latent codes and to set the token length of the output. 
Encoding/collecting latent codes is pretty quick is you use the modified code for the autoencoder and yaml files to override the token length used. < 0.1 sec per

Total meshes: 14,752
Total storage space for the pre-processed files: 1.6TB (includes the files shown at the end of the notes)
Total time to process the meshes: 5-6 days using the Blender based rendering scripts. 2-3 days using my fast_render scripts. 
renders + features = 368 GB total for 14,752 sets of 36 views using my updated scripts
Watertight/Normalizing
    - multi_threaded_to_watertight_mesh.py
    - About 1-2 days to finish this stage + the Sharp Edge Sampling. 
    - still need to update the script for better concurrent processing 

Sharp Edge Sampling
    - sharp_sample_wrapper.py + modified_sharp_sample.py 
    - looking at alternatives to blender for faster processing
    - zip_mesh_and_points.py  
        - I have added an archiving script to zip these up along with the wateright mesh
        - These files aren't used in the DIT training but I am keeping them for future experimenting
        - This should save 30-40% of the storage space for these files ("_normalized.obj", "_sharp_sample.ply", "_sample_points.npz")
Renders 
    - After using the blender based scripts and thinking it was too slow, I redid the rendering without blender. 
    - fast_render.py + view_utils.py + fast_render_wrapper.py (non Blender scripts - numpy, OpenGL, OpenCV, trimesh, and PIL)
        - These should get through the entire stack of meshes in 10% or less of the time that the Blender based script does (about 2 hours for my set of meshes)
        - The output from this script takes up only about 10% of the storage space since I'm saving those as png files
        - The render pops up on the screen during this so its kind of annoying, but its much faster
        - CPU usage is high for this, I'm running 12 workers with it. (90-100%)
        - GPU usage is around 25% with 12 workers running.
        - 1 hour 53 minutes 
        - I've added a script to use 7zip to compress the renders
            - They aren't going to be needed for the dit training but I am keeping them around for experimenting with
            - png files are already a compressed format, 7zip adds another 5-10% of compression which adds up with large data sets
            - helps keep the folder less chaotic
    - render_wrapper.py + render_script.py (These use Blender, which is kind of slow)
        - 2 1/2 days running with 8 workers 
        - I couldn't figure out how to push more of this to the 3090
        - The script as it is put my 3090 at around 20% usage
        - The integrated GPU was near 100% the entire time
        - The CPU usage sat pretty low
        - Might be faster with a different approach
        - Not sure about the normal map renders either, seems to be shadows in the scene for those
        - Renders are a bottleneck and take a lot more storage space than the other pre-processed parts
    - I went with 36 views for these, one render of the normal map, one render of the depth map for each view

Feature Extraction 
    - updated_feature_extraction  
        - 14752/14752 [1:10:16<00:00,  3.50folder/s]
        - 12 threads
        - using .npz vs .pt
        - used the dinov2 small pretrained model
    - feature_extraction.py 
        - 12 hours running with 8 threads 
        - I used this model: dinov2-with-registers-large

Latent Samples 
    - multi_thread_latent_extraction.py
    - 1 hour running with 8 threads 
    - Heaviest on the GPU - 100% usage but only around 5.5GB of VRAM
    - CPU was sitting high too off and on, 20% - 90%

Various folder and file organizing
    - move_npz_files.py
    - move_ply_files.py
    - copy_obj_to_render_folders.py
    - rename_folders.py
    - rename_meshes.py
    - rename_processed_files.py
    - fix_unmatched_claims.py - temporary script to fix up the race to process files in modified_sharp_sample.py, 
                                need to redo in a similar way to the render_wrapper to handle the queue of work

Various other scripts to test things
    - test_load.py (test loading the dinov2 model)
    - inspect_pt.py (test that the output from the dinov2 model has content)

TODO    
    - Design a DIT and training pipeline
    - Fine tune the pre-trained dora-VAE model 
    - Train a dora-VAE from scratch with my input meshes 
    - Train the DIT model 
    - Preprocess the meshes again at multiple higher resolutions for progressive training of the VAE and DIT models 
    - Combine all of the preprocessing scripts 
    - Get all of the input files paths / types from the various scripts in line with each other

Processed files folder structure (end goal would be to have multiple resolutions of all these for each mesh):

# d/processed/
# ├── meshName/
# │   ├── meshName.obj (original mesh, could be obj/stl/glb/ etc)
# │   └── 256_meshName/
# │       ├── 256_meshName_normalized.obj (processed mesh from watertight_mesh.py)
# │       ├── 256_meshName_sharp_sample.ply (output from sharp_edge_sampling)
# │       ├── 256_meshName_sample_points.npz (output from sharp_edge_sampling)
# │       ├── 256_meshName_latent_sample.npy (output from Dora-VAE - just the shape latents from the tuple) 
# │       ├── 256_meshName_view_000_dinov2_features.pt (one per view, npy for faster loading during training)
# │       ├── 256_meshName_view_000_depth_features.pt (from DINOv2 using the depth map render from this view)
# │       ├── 256_meshName_view_000_normal_features.pt (from DINOv2 using the normal map render from this view)
# │       ├── 256_meshName_view_000_depth.png (rendered depth map of the view)
# │       ├── 256_meshName_view_000_normal.png (rendered normal map of the view)
# │       ├── 256_meshName_view_001_depth.png
# │       ├── 256_meshName_view_001_normal.png
# │       ├── 256_meshName_view_001_depth_features.pt
# │       ├── 256_meshName_view_001_normal_features.pt 
# │       └── ... (up to 256_meshName_view_036_depth.png/normal.png/_features.npy/etc )
# ├── meshName2/
# │   ├── meshName2.obj
# │   └── 256_meshName2/
# │         └── ... 
# └── ... 






# d/data/
# ├── meshName/
# │   └── 256_meshName/
# │       ├── 256_meshName_latent_sample.npy (output from Dora-VAE - just the shape latents from the tuple) 
# │       ├── 256_meshName_view_000_depth_features.npz (from DINOv2 using the depth map render from this view)
# │       ├── 256_meshName_view_000_normal_features.npz (from DINOv2 using the normal map render from this view)
# │       ├── 256_meshName_view_001_depth_features.npz
# │       ├── 256_meshName_view_001_normal_features.npz 
# │       └── ... (up to 256_meshName_view_035_depth_features.npz/normal_features.npz)
# ├── meshName2/
# │   └── 256_meshName2/
# │         └── ... 
# └── ... 




